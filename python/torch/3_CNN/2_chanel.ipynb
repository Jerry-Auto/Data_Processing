{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e667a6b",
   "metadata": {},
   "source": [
    "多输入多输出通道  \n",
    "彩色图像具有标准的RGB通道来代表红、绿和蓝  \n",
    "当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。  \n",
    "例如，每个RGB输入图像具有的形状$3 \\times h \\times w$。  \n",
    "我们将这个大小为3的轴称为通道（channel）维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1253d",
   "metadata": {},
   "source": [
    "多输入通道  \n",
    "对每个通道执行互相关操作，然后将结果相加  \n",
    "卷积核张量形状为$c_i \\times k_h \\times k_w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23d395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "#K有三个维度\n",
    "def corr2d_multi_in(X, K):\n",
    "    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0233700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]),\n",
       " torch.Size([2, 2, 2]),\n",
       " tensor([[ 56.,  72.],\n",
       "         [104., 120.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "X.shape,K.shape,corr2d_multi_in(X, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4134899d",
   "metadata": {},
   "source": [
    "多输出通道  \n",
    "随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度  \n",
    "可以将每个通道看作对不同特征的响应。  \n",
    "而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。  \n",
    "因此，多输出通道并不仅是学习多个单通道的检测器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37519b6b",
   "metadata": {},
   "source": [
    "卷积核大小$k_h \\times k_w$  \n",
    "为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为$c_i \\times k_h \\times k_w$的卷积核张量  \n",
    "这样卷积核形状为$c_o \\times c_i \\times k_h \\times k_w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db600697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "    # 最后将所有结果都叠加在一起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32b7ae",
   "metadata": {},
   "source": [
    "通过将核张量K与K+1（K中每个元素加）和K+2连接起来，构造了一个具有3个输出通道的卷积核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "342a43e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20c90e",
   "metadata": {},
   "source": [
    "对输入张量X与卷积核张量K执行互相关运算。  \n",
    "现在的输出包含个3通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af385465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]),\n",
       " torch.Size([3, 2, 2, 2]),\n",
       " tensor([[[ 56.,  72.],\n",
       "          [104., 120.]],\n",
       " \n",
       "         [[ 76., 100.],\n",
       "          [148., 172.]],\n",
       " \n",
       "         [[ 96., 128.],\n",
       "          [192., 224.]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,K.shape,corr2d_multi_in_out(X, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce817771",
   "metadata": {},
   "source": [
    "$1 \\times 1$卷积核  \n",
    "这种卷积失去了卷积层的特有能力——在高度和宽度维度上，识别相邻元素间相互作用的能力  \n",
    "唯一计算发生在通道上  \n",
    "看作在每个像素位置应用的全连接层，以$c_i$个输入值转换为$c_o$个输出值。   \n",
    "因为这仍然是一个卷积层，所以跨像素的权重是一致的  \n",
    "需要的权重维度为$c_i \\times c_o$再额外加上一个偏置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3417b",
   "metadata": {},
   "source": [
    "使用全连接层实现1*1卷积。  \n",
    "请注意，我们需要对输入和输出的数据形状进行调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f2411d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    # 全连接层中的矩阵乘法\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef633a",
   "metadata": {},
   "source": [
    "上述函数相当于先前实现的互相关函数corr2d_multi_in_out。  \n",
    "让我们用一些样本数据来验证这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "543f05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274049b",
   "metadata": {},
   "source": [
    "汇聚层  \n",
    "希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。  \n",
    "而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），  \n",
    "所以我们最后一层的神经元应该对整个输入的全局敏感。  \n",
    "通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层  \n",
    "汇聚（pooling）层，它具有双重目的：  \n",
    "降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c2604",
   "metadata": {},
   "source": [
    "与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为汇聚窗口）遍历的每个位置计算一个输出。   \n",
    "然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。   \n",
    "相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。  \n",
    "这些操作分别称为最大汇聚层（maximum pooling）和平均汇聚层（average pooling）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588f358",
   "metadata": {},
   "source": [
    "汇聚窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动  \n",
    "汇聚窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。  \n",
    "计算最大值或平均值是取决于使用了最大汇聚层还是平均汇聚层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a544bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))#输出形状\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca51958",
   "metadata": {},
   "source": [
    "输入：\n",
    "$\\begin{bmatrix}0& 1 & 2 \\\\ 3 & 4 &5 \\\\6 & 7 & 8\\end{bmatrix}$  \n",
    "2*2 max pooling输出:  \n",
    "$\\begin{bmatrix} 4 & 5 \\\\ 7 & 8 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b11f44f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15dbf75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123df531",
   "metadata": {},
   "source": [
    "填充和步幅  \n",
    "与卷积层一样，汇聚层也可以改变输出形状。  \n",
    "和以前一样，我们可以通过填充和步幅以获得所需的输出形状  \n",
    "输入矩阵形状$n_h \\times n_w$  \n",
    "pooling形状$pl_h \\times pl_w$  \n",
    "stride $s_h \\times s_w$  \n",
    "pading $p_h \\times p_w$  \n",
    "输出形状：  \n",
    "$[(n_h-(pl_h-1)+2p_h+s_h-1)/s_h]\\times[(n_w-(pl_w-1)+2p_w+s_w-1)/s_w]$   \n",
    "当pading=0,输出为：  \n",
    "$[(n_h-(pl_h-1)+s_h-1)/s_h]\\times[(n_w-(pl_w-1)+s_w-1)/s_w]$  \n",
    "chanel保持不变  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "021935d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596238e",
   "metadata": {},
   "source": [
    "默认情况下，深度学习框架中的步幅与汇聚窗口的大小相同。   \n",
    "因此，如果我们使用形状为(3, 3)的汇聚窗口，那么默认情况下，我们得到的步幅形状为(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "483361c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3)\n",
    "pool2d(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a9d8942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee5eeb",
   "metadata": {},
   "source": [
    "多个通道  \n",
    "在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总  \n",
    "意味着汇聚层的输出通道数与输入通道数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "295b9d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.cat((X, X + 1), 1)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ddf1b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92910ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
