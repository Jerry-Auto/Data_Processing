{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4098369",
   "metadata": {},
   "source": [
    "介绍有关梯度，GPU,张量连接，内存的知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20f9a0",
   "metadata": {},
   "source": [
    "如果 out=f(x)是标量，out.backward() 会计算 ∂out/∂x  \n",
    "当out是矩阵时，out.backward() 默认不允许直接调用（因为梯度是矩阵，方向不明确）  \n",
    "必须提供一个与 out 形状相同的张量 gradient（即 grad_outputs），表示 out 的每个元素对最终损失的贡献权重。  \n",
    "torch.ones_like(out) 对 out 的每个元素求和（即 out.sum()）  \n",
    "对求和后的标量结果调用 backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6c69028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 24.],\n",
      "        [42., 64.]])\n",
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "y = torch.tensor([[5., 6.], [7., 8.]], requires_grad=True)#定义 x ，y并启用梯度跟踪\n",
    "out = (x.pow(2)*y).sum()#对x的每个元素平方，再对所有元素求和\n",
    "out.backward()#out最好是标量，不然很复杂\n",
    "print(x.grad)#表示OUT对x的梯度\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b2924",
   "metadata": {},
   "source": [
    "数据放到GPU或者CPU上进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "012d19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3], device='cuda:0')\n",
      "tensor([2, 3]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])#默认是放在cpu  \n",
    "if torch.cuda.is_available():         \n",
    "    device = torch.device('cuda')     \n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)                  \n",
    "    z = x + y                         \n",
    "    print(z)                          \n",
    "    print(z.to('cpu'), torch.double)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caebf07d",
   "metadata": {},
   "source": [
    "张量连结（concatenate）提供张量列表，给出沿哪个轴连结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d5308c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f36316",
   "metadata": {},
   "source": [
    "节省内存  \n",
    "Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac3ae2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb10ebd",
   "metadata": {},
   "source": [
    "执行原地更新，即Y的地址保持不变，只是更新上面的值  \n",
    "1.切片表示法Y[:] = <expression>\n",
    "2.+=操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ab945961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 125444138381728\n",
      "id(Z): 125444138381728\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "92c639e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y += X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a15743",
   "metadata": {},
   "source": [
    "下面详细剖析计算图的知识  \n",
    "<pre>\n",
    "计算图是其自动微分功能的核心，它本质上是一个​​有向无环图（DAG）​​，用于动态记录所有的张量操作，以便在反向传播时自动计算梯度\n",
    "其最大的特点是​​动态性​​，计算图是在代码运行时即时构建的，而非预先静态定义\n",
    "主要由以下两个基本元素构成：\n",
    "    ​节点 (Node)​​：代表​​数据​，​即​​张量（Tensor），节点分为叶子节点与非叶子节点\n",
    "        叶子节点：最底端的根\n",
    "            用户直接创建：如torch.tensor(..., requires_grad=True)。\n",
    "            无父节点：不依赖其他张量生成。\n",
    "            梯度存储：梯度存储在leaf_tensor.grad中。\n",
    "            使用is_leaf属性判断\n",
    "        非叶子节点：由叶子节点计算得到的中间节点(中间​张量数据)\n",
    "            梯度默认在反向传播后释放（内存优化）。\n",
    "            可通过retain_grad()强制保留。\n",
    "\n",
    "    边 (Edge)​​：代表​​操作（Operation）​​，即对张量进行的数学运算（如加法、乘法），连接着各个节点，方向表示数据流​​\n",
    "        连接了参与运算的输入张量（节点）和输出张量（节点）\n",
    "        边的方向指示了计算和梯度传播的方向。前向传播时，数据沿边指向的方向计算；反向传播时，梯度则沿边的反方向传播\n",
    "\n",
    "重要注意事项\n",
    "    ​​梯度累积与清零​​：由于梯度会累积在叶节点的 .grad中，在训练循环的每一步​​反向传播之前，需要手动将优化器的梯度清零​​（optimizer.zero_grad()），否则梯度会不断累加，导致训练不稳定\n",
    "    ​​禁用梯度跟踪​​：在进行模型推理或计算不需要梯度的中间值时，可以使用 with torch.no_grad():上下文管理器。这会暂时禁用计算图的构建，显著节省内存并加速计算\n",
    "    ​​分离张量​​：使用 .detach()方法可以从当前计算图中分离出一个张量。新张量与原始张量数据相同，但不再参与梯度计算，其 requires_grad为 False，通过设置为true会使其成为新的叶节点。\n",
    "    ​​原位操作（In-place）的限制​​：应避免对 requires_grad=True的​​叶节点​​进行原位操作（如 x.data += 1）。这会直接修改原始张量的值，可能会破坏计算图并导致梯度计算错误\n",
    "\n",
    "\n",
    "递归遍历打印计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3fdfeb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2_model_visualization.svg'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "# 创建测试计算图\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "z = x * y                  # MulBackward0\n",
    "w = x + y                  # AddBackward0\n",
    "out = z + w * 2            # AddBackward0 (包含两个输入)\n",
    "# 安全可视化计算图\n",
    "dot = make_dot(\n",
    "    out,  # 输出张量\n",
    "    params={#只能输入叶子张量进行重命名\n",
    "        \"叶子x\": x,\n",
    "        \"叶子y\": y\n",
    "    },\n",
    "    show_attrs=True\n",
    ")\n",
    "dot.render(\"2_model_visualization\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f034fd",
   "metadata": {},
   "source": [
    "backward()反向传播的必要条件：\n",
    "    存在从当前张量到叶子节点的完整计算图路径，  \n",
    "    而计算图的动态构建需满足张量参与运算且requires_grad=True，  \n",
    "    叶子节点则是由用户直接创建且requires_grad=True的张量  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46f8e0",
   "metadata": {},
   "source": [
    "计算示例  \n",
    "演示了叶子节点、中间节点、中间节点梯度保留、梯度分离、新建叶子节点等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "72e388bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 建立叶子节点\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([3.0])\n",
    "y.requires_grad=True\n",
    "\n",
    "#创建中间节点\n",
    "ind_a = x * y*2\n",
    "ind_b = x + y\n",
    "\n",
    "#保留一个中间节点的梯度\n",
    "ind_a.retain_grad()\n",
    "\n",
    "# 使用detach新建新分支\n",
    "det_a=ind_b.detach()\n",
    "print(det_a.requires_grad)#false\n",
    "# 分离出来的张量转换为叶子节点\n",
    "det_a.requires_grad_(True)\n",
    "\n",
    "#计算最终结果\n",
    "z=ind_a*ind_b+ind_b\n",
    "c=det_a*y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa566a13",
   "metadata": {},
   "source": [
    "进行backward计算结果节点对于叶子节点的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d1797a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True True\n",
      "True True False False False\n",
      "tensor([31.]) tensor([15.])\n",
      "tensor([4.]) None\n",
      "tensor([3.]) tensor([4.])\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41050/3445178771.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647352509/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(ind_a.grad,ind_b.grad)  # 非叶子节点默认释放，输出 None（除非使用 retain_grad）\n"
     ]
    }
   ],
   "source": [
    "#反向传播\n",
    "z.backward()  \n",
    "\n",
    "#验证所有节点的required_grad\n",
    "print(x.requires_grad,y.requires_grad,ind_a.requires_grad,ind_b.requires_grad,z.requires_grad)\n",
    "\n",
    "#验证是否是叶子节点\n",
    "print(x.is_leaf,y.is_leaf,ind_a.is_leaf,ind_b.is_leaf,z.is_leaf)\n",
    "\n",
    "#打印梯度\n",
    "print(x.grad,y.grad)  \n",
    "print(ind_a.grad,ind_b.grad)  # 非叶子节点默认释放，输出 None（除非使用 retain_grad）\n",
    "\n",
    "#梯度清零，避免累加\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "ind_a.grad.zero_()\n",
    "#反向传播\n",
    "c.backward()\n",
    "print(det_a.grad,y.grad)\n",
    "print(det_a.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d6e47",
   "metadata": {},
   "source": [
    "如果要再次计算梯度，首先进行梯度清零，然后重新通过计算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb04885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31.]) tensor([15.])\n",
      "tensor([4.]) None\n",
      "tensor([3.]) tensor([4.])\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41050/1648224921.py:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647352509/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(ind_a.grad,ind_b.grad)  # 非叶子节点默认释放，输出 None（除非使用 retain_grad）\n"
     ]
    }
   ],
   "source": [
    "#梯度清零，避免累加\n",
    "det_a.grad.zero_()\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "ind_a.grad.zero_()\n",
    "\n",
    "#再次经过计算过程\n",
    "ind_a = x * y*2\n",
    "ind_a.retain_grad()\n",
    "ind_b = x + y\n",
    "z=ind_a*ind_b+ind_b\n",
    "#再次backward\n",
    "z.backward()  \n",
    "#打印梯度\n",
    "print(x.grad,y.grad)  \n",
    "print(ind_a.grad,ind_b.grad)  # 非叶子节点默认释放，输出 None（除非使用 retain_grad）\n",
    "\n",
    "#梯度清零，避免累加\n",
    "det_a.grad.zero_()\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "ind_a.grad.zero_()\n",
    "\n",
    "c=det_a*y\n",
    "#反向传播\n",
    "c.backward()\n",
    "print(det_a.grad,y.grad)\n",
    "print(det_a.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b5455f",
   "metadata": {},
   "source": [
    "with torch.no_grad()  与y=x.detach()的异同：  \n",
    "<pre>\n",
    "with torch.no_grad()  \n",
    "    是一个上下文管理器，用于临时禁用整个代码块内的梯度计算。在该块内，所有涉及张量的操作（如前向传播、数学运算）都不会构建计算图，也不会记录梯度。  \n",
    "    本质：通过全局设置 torch.set_grad_enabled(False) 阻断计算图的继续生成，节省内存和计算资源。\n",
    "y=x.detach()\n",
    "    是一个张量方法，用于从当前计算图中分离出一个张量副本。返回的新张量y与原张量共享数据（内存地址相同），但切断梯度传播路径，即反向传播时不会通过该张量回溯到原始计算图。\n",
    "    修改原张量的数值会影响 detach 后的张量（反之亦然）  \n",
    "    分离后的张量 requires_grad 属性为 false，分离后导致计算图断裂：\n",
    "        如果需要跟踪返回的变量，需要显示设定required_grad为true使之成为叶子节点，再进行后续计算的跟踪和新计算图构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceddffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
