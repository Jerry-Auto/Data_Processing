{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f315c5dd",
   "metadata": {},
   "source": [
    "介绍参数管理(parameters)  \n",
    "\n",
    "    访问参数，用于调试、诊断和可视化；\n",
    "\n",
    "    参数初始化；\n",
    "\n",
    "    在不同模型组件间共享参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aae64e",
   "metadata": {},
   "source": [
    "训练阶段我们的目标是找到使损失函数最小化的模型参数值  \n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。  \n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们   \n",
    "将模型保存下来，以便它可以在其他软件中执行， 或者为了获得科学的理解而进行检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234aafdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0901],\n",
       "        [0.1023]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdab531",
   "metadata": {},
   "source": [
    "从已有模型中访问参数  \n",
    "当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。  \n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1e86ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0521,  0.2268, -0.3199,  0.3117, -0.3089,  0.2409, -0.0738, -0.0154]])), ('bias', tensor([0.1458]))])\n"
     ]
    }
   ],
   "source": [
    "#检查第二个全连接层的参数\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42000c",
   "metadata": {},
   "source": [
    "全连接层包含两个参数，分别是该层的权重和偏置。  \n",
    "两者都存储为单精度浮点数（float32）。  \n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5e112",
   "metadata": {},
   "source": [
    "每个参数都表示为参数类的一个实例。  \n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。  \n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。  \n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b96c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1458], requires_grad=True)\n",
      "tensor([0.1458])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e165c32",
   "metadata": {},
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。   \n",
    "除了值之外，我们还可以访问每个参数的梯度。  \n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd2494d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad==None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd13017",
   "metadata": {},
   "source": [
    "一次性访问所有参数  \n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。  \n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂， 因为我们需要递归整个树来提取每个子块的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80ef435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b6e65",
   "metadata": {},
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88124d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1458])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9da60",
   "metadata": {},
   "source": [
    "从嵌套块收集参数  \n",
    "如果我们将多个块相互嵌套，参数命名约定是如何工作的。  \n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea352548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2015],\n",
       "        [0.2011]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335afebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#设计了网络后，我们看看它是如何工作的\n",
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553426f",
   "metadata": {},
   "source": [
    "层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。  \n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43793ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0400,  0.3567, -0.0385,  0.2286,  0.1470, -0.2588,  0.4917, -0.3012])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b97005",
   "metadata": {},
   "source": [
    "参数初始化  \n",
    "看看如何正确地初始化参数  \n",
    "深度学习框架提供默认随机初始化， 也允许我们创建自定义初始化方法， 满足我们通过其他规则实现初始化权重。  \n",
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。   \n",
    "PyTorch的nn.init模块提供了多种预置初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e81801f",
   "metadata": {},
   "source": [
    "内置的初始化器  \n",
    " 下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10919646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0043,  0.0100,  0.0076, -0.0164]), tensor(0.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa4111",
   "metadata": {},
   "source": [
    "还可以将所有参数初始化为给定的常数，比如初始化为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b287219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a91af3",
   "metadata": {},
   "source": [
    "还可以对某些块应用不同的初始化方法。  \n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3e5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6298, -0.3628,  0.3677, -0.0098])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74755a76",
   "metadata": {},
   "source": [
    "自定义初始化  \n",
    "有时，深度学习框架没有提供我们需要的初始化方法。  \n",
    "在下面的例子中，我们使用以下的分布为任意权重参数w定义初始化方法：\n",
    "$$\n",
    "w \\sim \\begin{cases} \n",
    "U(5, 10) & \\text{可能性 } \\frac{1}{4} \\\\ \n",
    "0 & \\text{可能性 } \\frac{1}{2} \\\\ \n",
    "U(-10, -5) & \\text{可能性 } \\frac{1}{4} \n",
    "\\end{cases}\n",
    "$$\n",
    "同样，我们实现了一个my_init函数来应用到net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6529ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000,  8.8908, -6.2080],\n",
       "        [ 7.2066, -8.4546,  9.4437,  8.5047]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8290e",
   "metadata": {},
   "source": [
    "始终可以直接设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6b1ee00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  9.8908, -5.2080])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd1685b",
   "metadata": {},
   "source": [
    "参数绑定  \n",
    "希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b645f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed4ba7",
   "metadata": {},
   "source": [
    "这个例子表明第三个和第五个神经网络层的参数是绑定的。  \n",
    "它们不仅值相等，而且由相同的张量表示。  \n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。  \n",
    "这里有一个问题：当参数绑定时，梯度会发生什么情况？  \n",
    "答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd62113",
   "metadata": {},
   "source": [
    "延后初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2924da7e",
   "metadata": {},
   "source": [
    "<pre>到目前为止，我们忽略了建立网络时需要做的以下这些事情：\n",
    "\n",
    "    我们定义了网络架构，但没有指定输入维度。\n",
    "\n",
    "    我们添加层时没有指定前一层的输出维度。\n",
    "\n",
    "    我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。\n",
    "\n",
    "有些读者可能会对我们的代码能运行感到惊讶。  \n",
    "毕竟，深度学习框架无法判断网络的输入维度是什么。  \n",
    "这里的诀窍是框架的延后初始化（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。\n",
    "\n",
    "在以后，当使用卷积神经网络时， 由于输入维度（即图像的分辨率）将影响每个后续层的维数， 有了该技术将更加方便。  \n",
    "现在我们在编写代码时无须知道维度是什么就可以设置参数， 这种能力可以大大简化定义和修改模型的任务。  \n",
    "接下来，我们将更深入地研究初始化机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8105b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(),nn.Linear(256,10))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b4073",
   "metadata": {},
   "source": [
    "此时，因为输入维数是未知的，所以网络不可能知道输入层权重的维数。 因此，框架尚未初始化任何参数，我们通过尝试访问以下参数进行确认。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe779d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('weight', <UninitializedParameter>),\n",
       "              ('bias', <UninitializedParameter>)]),\n",
       " OrderedDict(),\n",
       " OrderedDict([('weight',\n",
       "               tensor([[ 0.0449,  0.0394, -0.0511,  ...,  0.0039,  0.0350, -0.0249],\n",
       "                       [ 0.0485,  0.0060,  0.0228,  ..., -0.0566,  0.0102, -0.0506],\n",
       "                       [ 0.0361, -0.0016,  0.0534,  ..., -0.0305, -0.0582,  0.0435],\n",
       "                       ...,\n",
       "                       [ 0.0342,  0.0600,  0.0021,  ...,  0.0354, -0.0064, -0.0341],\n",
       "                       [-0.0012, -0.0601,  0.0427,  ..., -0.0443, -0.0520,  0.0045],\n",
       "                       [ 0.0008,  0.0589,  0.0388,  ..., -0.0622,  0.0411,  0.0374]])),\n",
       "              ('bias',\n",
       "               tensor([ 0.0237, -0.0101, -0.0477, -0.0237, -0.0096,  0.0117,  0.0310,  0.0513,\n",
       "                       -0.0368,  0.0448]))])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[net[i].state_dict() for i in range(len(net))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204778c",
   "metadata": {},
   "source": [
    "接下来让我们将数据通过网络，最终使框架初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50bba30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "low = torch.finfo(torch.float32).min/10\n",
    "high = torch.finfo(torch.float32).max/10\n",
    "X = torch.zeros([2,20],dtype=torch.float32).uniform_(low, high)\n",
    "\n",
    "net(X)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac52b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
